{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'> BRYN MAWR COLLEGE\n",
    "\n",
    "<font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'><b>Computational Methods in the Physical Sciences</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><b>Module 7:  Eigenquations</b><br/><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Prerequisite modules***: Module 6 -- Linear Equations \n",
    "\n",
    "***Estimated completion time***: 5-8 hours  \n",
    "\n",
    "***Learning objectives***: Learn how eigenvectors and eigenvalues are found numerically, and explore some scientifically relevant applications \n",
    "\n",
    "***Note:*** This module requires some familiarity with linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/Fairy_tales_ref_to_eigenvectors_xkcd.png'  width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>(Image credit: xkcd.com.  See http://www.explainxkcd.com/wiki/index.php/File:fairy_tales.png for an explanation.)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module$^1$ can be thought of as the second of a pair of modules exploring computational matrix methods, Module 6 being the first in the series.  Here, we explore numerical methods for determining eigenvalues and eigenvectors of matrices.  This ability is useful in classical mechanics for the determination of principal axes of rotation and their corresponding moments of inertia, and for understanding the motions of coupled oscillators; in quantum mechanics for the identification of the energy values and states of a system; in engineering, for predicting oscillatory motion of structures (e.g., in reaction to earthquakes) and for the design of MEMS (microelectromechanical systems) devices; and even in biology, for the study of protein conformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Scientist Profile</font>\n",
    "\n",
    "**Mark Dean** is an inventor and computer engineer, born on March 2, 1957 in Jefferson City, Tennessee.  Dr. Dean earned a bachelor’s degree in Electrical Engineering from the University of Tennessee, a master’s in Electrical Engineering from Florida Atlantic University, and a Ph.D. in Electrical Engineering from Stanford University.  Upon graduating from college, Dean soon started working at a young IBM, where he rose to become the first African American IBM Fellow, the highest recognition of technical excellence at the company.  In his time at IBM, where he spent the majority of his career, Dean became famous for three main contributions to the worlds of computer science and computer engineering: he helped develop the Industry Standard Architecture (ISA) systems bus, which allowed outside devices like the mouse and disk drives to be plugged directly into the computer; some of his work also helped lead to the development of the first color PC monitor; and, finally, he is known for leading the IBM team that created the first gigahertz processing chip.  As evidence of his influence on the field, Dean holds three of IBM’s original nine patents, and he has more than 20 patents in total to his name.  His inspirational story as a pioneering African American in the sciences has been and continues to be a source of inspiration for aspiring computer scientists of all races."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/Mark_Dean.jpg'  width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">7.1  Eigenvalues and Eigenvectors</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general problem of interest in this module has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v},  \\hspace{50pt}  (1)  %%\\label{ev}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{A}$ is a matrix, the vectors $\\mathbf{v}$ that solve the equation are called ***eigenvectors***, and the corresponding scalar $\\lambda$ values are ***eigenvalues***.  The eigenvectors represent vectors whose directions are not changed by the action of the matrix.  (The eigenvalues indicate how much the magnitudes are changed.)  An $N \\times N$ matrix $\\mathbf{A}$ will have $N$ eigenvectors, which will be labeled $\\mathbf{v}_i$, and $N$ corresponding eigenvalues $\\lambda_i$ (some of which may be the same; i.e., \"degenerate\").  \n",
    "\n",
    "It will turn out to be useful to define a new matrix $\\mathbf{v}$ whose columns are the eigenvectors ($\\mathbf{v}_1$ is the first column, $\\mathbf{v}_2$ is the second column, etc.), and also turn the eigenvalues into a *diagonal* matrix $\\mathbf{\\Lambda}$ (so  $\\mathbf{\\Lambda}_{11} = \\lambda_1$, $\\mathbf{\\Lambda}_{22} = \\lambda_2$, etc., and all non-diagonal elements of $\\mathbf{\\Lambda}$ are zero).  By writing out the multiplications indicated in Eq. (1), it's not too hard to show that in terms of these new matrices the original problem can be expressed as  \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A} \\mathbf{V} = \\mathbf{V} \\mathbf{\\Lambda} .   \\hspace{50pt}  (2)   %%\\label{mev}\n",
    "\\end{equation}\n",
    "\n",
    "The usual approach to solving Eq. (1) involves rewriting the equation as $(\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{v} = \\mathbf{0}$, where $\\mathbf{I}$ is the $N \\times N$ unit matrix and $\\mathbf{0}$ is an $N$-element vector of zeros.  A nontrivial solution can be found only if $|\\mathbf{A} - \\lambda \\mathbf{I}| = \\mathbf{0}$.  Imposing that requirement leads to an algebraic ***characteristic equation*** in terms of the eigenvalues.  Once those are found, the eigenvectors then can be found by substitution of the eigenvalues back into Eq. (1).  \n",
    "\n",
    "As we've seen in other situations, the usual analytical approach in this case is not an efficient one for finding a numerical solution to the problem.  One category of efficient numerical approaches is based on factorization of the matrix $\\mathbf{A}$.  A common factorization method for finding eigenvalues which we will explore here --- specifically for the case of a *real*, *symmetric* $\\mathbf{A}$ (whose eigenvalues are real) --- is the ***QR algorithm*** mentioned in Module 6.  This involves writing $\\mathbf{A}$ as a product of an ***orthogonal*** matrix $\\mathbf{Q}$ (obeying $\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{Q}^{-1} \\mathbf{Q} = \\mathbf{I}$, where $\\mathbf{Q}^T$ is the transpose of $\\mathbf{Q}$ and $\\mathbf{I}$ is the identity matrix), and an upper-triangular matrix $\\mathbf{R}$, in what is termed a ***QR decomposition***, which always is possible for a square matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A} = \\mathbf{Q}_1 \\mathbf{R}_1 .\n",
    "\\end{equation}\n",
    "  \n",
    "If we multiply on the left by $\\mathbf{Q}_1^T$, then by orthogonality we get\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Q}_1^T \\mathbf{A} = \\mathbf{Q}_1^T \\mathbf{Q}_1 \\mathbf{R}_1 = \\mathbf{I} \\mathbf{R}_1  = \\mathbf{R}_1 .     \n",
    "\\hspace{50pt}  (3)  %%\\label{R1}\n",
    "\\end{equation}\n",
    "\n",
    "We now define a new matrix $\\mathbf{A}_1$ as the reverse product of the two matrices used to factorize $\\mathbf{A}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A}_1 = \\mathbf{R}_1 \\mathbf{Q}_1 .  \\hspace{50pt}  (4)  %\\label{RQ}\n",
    "\\end{equation}\n",
    "\n",
    "Using Eq. (3), we can rewrite this as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A}_1 = \\mathbf{Q}_1^T \\mathbf{A} \\mathbf{Q}_1 .   \\hspace{50pt}  (5)  %\\label{QAQ}\n",
    "\\end{equation}\n",
    "\n",
    "This process is now repeated, by constructing the QR decomposition of $\\mathbf{A}_1$ in terms of a new orthogonal matrix $\\mathbf{Q}_2$ and new upper-triangular matrix $\\mathbf{R}_2$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A}_1 = \\mathbf{Q}_2 \\mathbf{R}_2 .  \\hspace{50pt}  (6)   %\\label{A1def}\n",
    "\\end{equation}\n",
    "\n",
    "Again define a new matrix as the reverse product of the new matrices,\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A}_2 = \\mathbf{R}_2 \\mathbf{Q}_2 = \\mathbf{Q}_2^T \\mathbf{A}_1 \\mathbf{Q}_2 ,\n",
    "\\end{equation}\n",
    "\n",
    "where in the last step we used Eq. (6) left-multiplied by $\\mathbf{Q}_2^T$ ($= \\mathbf{Q}_2^{-1}$).  Expanding $\\mathbf{A}_1$ using Eq. (5), obtain\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A}_2 = \\mathbf{Q}_2^T \\mathbf{Q}_1^T \\mathbf{A} \\mathbf{Q}_1 \\mathbf{Q}_2 .  \n",
    "\\end{equation}\n",
    "\n",
    "Repeating the process further, we can construct a sequence of matrices:\n",
    "\n",
    "\\begin{align}\n",
    "\t\\mathbf{A}_1 & = \\mathbf{Q}_1^T \\mathbf{A} \\mathbf{Q}_1  \\nonumber \\\\  \n",
    "\t\\mathbf{A}_2 & = \\mathbf{Q}_2^T \\mathbf{Q}_1^T \\mathbf{A} \\mathbf{Q}_1 \\mathbf{Q}_2  \\nonumber \\\\\n",
    "\t& \\vdots  \\nonumber \\\\\n",
    "\t\\mathbf{A}_n & = \\left( \\mathbf{Q}_n^T \\dots \\mathbf{Q}_2^T \\mathbf{Q}_1^T \\right) \\mathbf{A} \n",
    "\t\\left( \\mathbf{Q}_1 \\mathbf{Q}_2 \\dots \\mathbf{Q}_n \\right) .   \\hspace{50pt}  (7)  %\\label{An}\n",
    "\\end{align} \n",
    "\n",
    "What's the point of all of this?  Well, it turns out that if this process is continued to large enough $n$, then $\\mathbf{A}_n$ will be diagonal, or nearly so; i.e., the off-diagonal elements can be made as small as desired.  Thus, we can write $\\mathbf{A}_n \\simeq \\mathbf{D}$, with $\\mathbf{D}$ diagonal.  To make use of this fact, we define a new matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{V} = \\mathbf{Q}_1 \\mathbf{Q}_2 \\dots \\mathbf{Q}_n = \\prod_{i = 1}^k \\mathbf{Q}_i ,  \\hspace{50pt}  (8)   %%\\label{Vdef}\n",
    "\\end{equation}\n",
    "\n",
    "which itself will be an orthogonal matrix since all of the $\\mathbf{Q}_i$ are orthogonal.  In terms of this matrix, Eq. (7) becomes\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{D} \\simeq \\mathbf{A}_n = \\mathbf{V}^T \\mathbf{A} \\mathbf{V} .   \\hspace{50pt}  (9)  %%\\label{Ddef}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, multiplying on the left by $(\\mathbf{V}^T)^{-1} = \\mathbf{V}$ and exchanging the two sides of the equation gives\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A} \\mathbf{V} = \\mathbf{V} \\mathbf{D} .  \n",
    "\\end{equation}\n",
    "\n",
    "This is identical to the matrix version of the eigenvalue equation seen in Eq. (2), if we identify $\\mathbf{\\Lambda} = \\mathbf{D}$.  Therefore, the eigenvectors of $\\mathbf{A}$ are the columns of \\mathbf{v} as defined in Eq. (8), and the desired eigenvalues are the (diagonal) elements of $\\mathbf{A}_n$.  The game, then, is to find the $\\mathbf{Q}_i$ matrices, construct \\mathbf{v} from Eq. (8), and then get $\\mathbf{D}$ from Eq. (9).  \n",
    "\n",
    "A method for finding the $\\mathbf{Q}_i$ matrices, which may be familiar from linear algebra, is based on the ***Gram-Schmidt orthogonalization*** (GSO) procedure used to construct a set of orthogonal vectors from a set of linearly independent ones.  Recall that the idea is to start with two of the vectors, letting one --- label it $\\mathbf{v}_1$ --- be the starting point.  The corresponding unit vector would be $\\mathbf{e}_1 \\equiv \\mathbf{v}_1 / \\left| \\mathbf{v}_1 \\right|$.  To make the second vector (call it $\\mathbf{v}_2$) orthogonal to the first, we subtract from $\\mathbf{v}_2$ its component along $\\mathbf{v}_1$, giving us a new second vector: $\\mathbf{v}'_2 = \\mathbf{v}_2 - \\left(\\mathbf{v}_2 \\cdot \\mathbf{e}_1 \\right) \\mathbf{e}_1$.  To construct a third orthogonal vector, $\\mathbf{v}'_3$, we need to subtract off the components of the third original vector $\\mathbf{v}_3$ that lie along $\\mathbf{v}_1$ and $\\mathbf{v}'_2$, and so on.  If we wish these vectors to be normalized, we divide each of the resulting vectors by its magnitude.  \n",
    "\n",
    "To use GSO in the present application we start from the matrix $\\mathbf{A}$ written as a set of column vectors, $\\mathbf{a}_i$:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A} = \n",
    "\t\\begin{bmatrix}\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{a}_1  \\\\\n",
    "\t\t\t\\vert\n",
    "\t\t\\end{pmatrix}\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{a}_2 \\\\\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\\end{pmatrix}\n",
    "\t\t\\cdots  \t\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{a}_N \\\\\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\\end{pmatrix}\n",
    "\t\\end{bmatrix} .  \n",
    "\\end{equation}  \n",
    "\n",
    "We next define a new set of mutually-orthogonal vectors, and corresponding unit vectors, as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\t& \\mathbf{w}_1 = \\mathbf{a}_1 ,  \\hspace{20mm}  & \\mathbf{e}_1 = \\mathbf{w}_1 / |\\mathbf{w}_1| ; \\notag \\\\\n",
    "\t& \\mathbf{w}_2 = \\mathbf{a}_2 - \\left( \\mathbf{a}_2 \\cdot \\mathbf{e}_1 \\right) \\mathbf{e}_1 , \\hspace{20mm} & \\mathbf{e}_2 = \\mathbf{w}_2 / |\\mathbf{w}_2| ;  \\notag \\\\   \n",
    "\t& \\mathbf{w}_3 = \\mathbf{a}_3 - \\left( \\mathbf{a}_3 \\cdot \\mathbf{e}_1 \\right) \\mathbf{e}_1  - \\left( \\mathbf{a}_3 \\cdot \\mathbf{e}_2 \\right) \\mathbf{e}_2 , \\hspace{20mm}  & \\mathbf{e}_3 = \\mathbf{w}_3 / |\\mathbf{w}_3| ; \\notag \\\\\n",
    "\t& \\hspace{6mm} \\vdots   & \\vdots  \\hspace{15mm}  \\hspace{50pt}  (10)  %\\label{weeqns}\n",
    "\\end{align}  \n",
    "\n",
    "Note that each $\\mathbf{w}_i$ depends only on the $\\mathbf{a}_j$ with $j = i$, and the $\\mathbf{e}_j$'s with $j < i$; therefore, the $\\mathbf{w}$'s can be built up sequentially, starting from $\\mathbf{w}_1$.  \n",
    "\n",
    "Rearranging the preceding expressions, we can write\n",
    "\n",
    "\\begin{align}\n",
    "\t\\mathbf{a}_1  & = |\\mathbf{w}_1| \\, \\mathbf{e}_1 ,  \\\\\n",
    "\t\\mathbf{a}_2  & = |\\mathbf{w}_2| \\, \\mathbf{e}_2 + \\left( \\mathbf{a}_2 \\cdot \\mathbf{e}_1 \\right) \\mathbf{e}_1 ,  \\\\\n",
    "\t\\mathbf{a}_3  & = |\\mathbf{w}_3| \\, \\mathbf{e}_3 + \\left( \\mathbf{a}_3 \\cdot \\mathbf{e}_1 \\right) \\mathbf{e}_1  + \\left( \\mathbf{a}_3 \\cdot \\mathbf{e}_2 \\right) \\mathbf{e}_2 ,  \\\\\n",
    "\t& \\hspace{2mm} \\vdots  \n",
    "\\end{align}\n",
    "\n",
    "These equations can be rewritten as the following matrix equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{A} = \n",
    "\t\\begin{bmatrix}\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{a}_1  \\\\\n",
    "\t\t\t\\vert\n",
    "\t\t\\end{pmatrix}\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{a}_2 \\\\\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\\end{pmatrix}\n",
    "\t\t\\cdots  \t\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{a}_N \\\\\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\\end{pmatrix}\n",
    "\t\\end{bmatrix} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{e}_1  \\\\\n",
    "\t\t\t\\vert\n",
    "\t\t\\end{pmatrix}\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{e}_2 \\\\\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\\end{pmatrix}\n",
    "\t\t\\cdots  \t\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{e}_N \\\\\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\\end{pmatrix}\n",
    "\t\\end{bmatrix} \n",
    "\t\\begin{pmatrix}\n",
    "\t\t|\\mathbf{w}_1| & \\mathbf{a}_2 \\cdot \\mathbf{e}_1 & \\mathbf{a}_3 \\cdot \\mathbf{e}_1  & \\dots \\\\\n",
    "\t\t0       & |\\mathbf{w}_2|                     & \\mathbf{a}_3 \\cdot \\mathbf{e}_2  & \\dots \\\\ \n",
    "\t\t0\t    & 0  \t\t\t\t\t\t  & |\\mathbf{w}_3|\t\t\t\t\t\t & \\dots \\\\\n",
    "\t\t\\vdots  & \\vdots\t\t\t\t\t  & \\vdots\t\t\t\t\t\t \n",
    "\t\\end{pmatrix} .  \n",
    "\\end{equation}\n",
    "  \n",
    "The first matrix on the right is orthogonal because the column vectors $\\mathbf{e}_i$ are orthogonal, and the second matrix is of upper-triangular form.  Therefore, we have succeeded in factoring $\\mathbf{A}$ into the form $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$, where\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\mathbf{Q} = \n",
    "\t\\begin{bmatrix}\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{e}_1  \\\\\n",
    "\t\t\t\\vert\n",
    "\t\t\\end{pmatrix}\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{e}_2 \\\\\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\\end{pmatrix}\n",
    "\t\t\\cdots  \t\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\t\\mathbf{e}_N \\\\\n",
    "\t\t\t\\vert  \\\\\n",
    "\t\t\\end{pmatrix}\n",
    "\t\\end{bmatrix}   \n",
    "\t\\hspace{10mm} \\text{and} \\hspace{10mm}\n",
    "\t\\mathbf{R} = \n",
    "\t\\begin{pmatrix}\n",
    "\t\t|\\mathbf{w}_1| & \\mathbf{a}_2 \\cdot \\mathbf{e}_1 & \\mathbf{a}_3 \\cdot \\mathbf{e}_1  & \\dots \\\\\n",
    "\t\t0       & |\\mathbf{w}_2|                & \\mathbf{a}_3 \\cdot \\mathbf{e}_2  & \\dots \\\\ \n",
    "\t\t0\t    & 0  \t\t\t\t     & |\\mathbf{w}_3|\t\t\t\t   & \\dots \\\\\n",
    "\t\t\\vdots  & \\vdots\t\t\t\t & \\vdots\n",
    "\t\\end{pmatrix} .   \\hspace{50pt}  (11)   %\\label{QRmatrices}\n",
    "\\end{equation}\n",
    "\n",
    "Putting the pieces together, the complete QR algorithm is as follows:\n",
    "\n",
    "* Create an $N \\times N$ matrix $\\mathbf{v}$ to hold the eigenvectors and set it equal to the identity matrix $\\mathbf{I}$.  Additionally, choose an accuracy threshold $\\epsilon$ for the off-diagonal elements of $\\mathbf{v}$. \n",
    "\t\n",
    "* For a given matrix $\\mathbf{A}$, compute the QR decomposition $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$ by determining the $\\mathbf{w}_i$ and corresponding $\\mathbf{e}_i$ vectors as in Eqs. (10), and forming the matrices in Eqs. (11).\n",
    "\t\n",
    "* As in Eq. (4), update $\\mathbf{A}$ to the new value $\\mathbf{A} = \\mathbf{R} \\mathbf{Q}$, using the $\\mathbf{Q}$ and $\\mathbf{R}$ found in step 3.\n",
    "\t\n",
    "* Update $\\mathbf{v}$ by computing $\\mathbf{V} \\mathbf{Q}$ using the latest $\\mathbf{Q}$, as in Eq. (8).\n",
    "\t\n",
    "* Check if all the off-diagonal elements of $\\mathbf{A}$ are less than $\\epsilon$; if so, end; if not, return to step 3.\n",
    "\n",
    "Upon completion of this algorithm, the diagonal elements of $\\mathbf{A}$ are the desired eigenvalues and the columns of $\\mathbf{v}$ are the eigenvectors.\n",
    "\n",
    "Implementing the full algorithm in Python is doable but time-consuming and, as might be expected, Python has its own functions to perform these computations.  They are in the `numpy.linalg` package and go by the names `eigh` and `eigvalsh` (the \"`h`\"'s at the ends of the function names refer to \"Hermitian\").  `eigh` calculates both the eigenvalues and eigenvectors of Hermitian matrices (of which real symmetric matrices are a subset), and is called as `e,V = eigh(A)`, where `e` is a 1-D array of eigenvalues and `V` is a 2-D array whose columns are the corresponding eigenvectors.  If one wants only the eigenvalues of a large matrix it is better to use `eigvalsh`, since it runs much faster than `eigh`.  It outputs just a 1-D array of the eigenvalues.  (Note: Python also has functions `eig` and `eigvals` to deal with non-Hermitian matrices, but problems involving such matrices are not common in physics.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Breakpoint 1</b></font>: Using GSO as presented in Eqs. (10), analytically create a set of normalized, orthogonal vectors from the set $\\mathbf{u}_1 = (1, -1, 1)$, $\\mathbf{u}_2 = (1, 0, 1)$,  $\\mathbf{u}_3 = (1, 1, 2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">7.2  Applications</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we consider a few situations of physical interest that can be analyzed using the eigenvalue/vector-finding techniques of the previous section.  (There are, of course, many other examples.)  The Exercises will give you some practice implementing the approaches presented below.\n",
    "\n",
    "#### <font color=\"blue\"> 7.2.1  Principal Axes of Inertia\n",
    "\n",
    "For a body rotating about a point *O*, its angular momentum $\\mathbf{L}$ will *not* in general be parallel to the axis of rotation, as specified by a vector $\\boldsymbol{\\omega}$ (the magnitude $|\\boldsymbol{\\omega}| = \\omega$ is the angular speed, and the direction of $\\boldsymbol{\\omega}$ is parallel to the axis of rotation).  However, for every body there exist (at least) three (perpendicular) axes along which the angular momentum vector and the rotation axis can remain coincident; such an axis is termed a ***principal axis***.  (For some bodies, e.g. a uniform sphere, there may be many such axes, possibly even an infinite number.)  In this case, we can write $\\mathbf{L} = \\lambda \\boldsymbol{\\omega}$, for some real number $\\lambda$.  We also know that the angular momentum can be written in terms of the ***moment of inertia tensor***, $\\mathbf{I}$, as $\\mathbf{L} = \\mathbf{I} \\boldsymbol{\\omega}$ (the rotational version of $\\mathbf{p} = m \\mathbf{v}$).  Putting these two facts together, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{I} \\boldsymbol{\\omega} = \\lambda \\boldsymbol{\\omega},\n",
    "\\end{equation}\n",
    "\n",
    "which is an eigenvalue equation.  The eigenvectors $\\boldsymbol{\\omega}$ of this equation indicate the axes around which the body can rotate stably, and the corresponding eigenvalues $\\lambda$ are the moments of inertia for rotation about those axes.  \n",
    "\n",
    "\n",
    "#### <font color=\"blue\">7.2.2  Coupled Harmonic Oscillators\n",
    "\n",
    "An important physical problem for which we might want to calculate eigenvalues and eigenvectors involves a collection of masses connected by springs (this is a reasonable model for a solid in which the masses are atoms and the springs represent the interatomic forces).  Since this type of problem is treated in all advanced classical mechanics texts, the present discussion will omit the derivations of the relevant equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/Coupled_Harmonic_Oscillators.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Figure 1**: Two 1-D coupled harmonic oscillators</center>\n",
    "<br>\n",
    "<center>(Image credit: Wikipedia page on coupled oscillators.)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this discussion, we'll consider a linear (1-D) arrangement of masses (see Figure 1 for one of the simplest cases of two coupled oscillators), but the general procedure can be extended in a straightforward way to 2-D or 3-D arrangements.  Each spring has an *equilibrium length* when no forces act to stretch or compress it; each of the $N$ masses in the system, $m_i$, has a corresponding equilibrium position when the springs to which it is attached are at their equilibrium lengths.  If a mass is displaced from its equilibrium position by distance $x_i$, the length of an attached spring will be changed away from its equilibrium length (unless the other attached mass is displaced identically).  In the process, the spring will gain potential energy $V = \\tfrac{1}{2} k d^2$, where $k$ is its spring constant and $d$ is the distance by which the spring length has been changed from its equilibrium length ($d$ is negative for compression, positive for extension).  A spring will change length as a result of movement of either or both of the masses that it connects; in the 1-D case, the potential energy associated with the spring connecting the $i^{\\text{th}}$ and $j^{\\text{th}}$ masses is $V_{ij} = \\tfrac{1}{2} k_{ij} (x_i - x_j)^2$, where $k_{ij}$ is the *spring constant* of the spring.  In the case of a spring with one end attached to a fixed point, like the two outer springs in the figure, the potential energy depends only on the displacement of the single attached mass.  \n",
    "\n",
    "For the system shown in the figure, the potential energy resulting from displacements $x_1$ and $x_2$ of the two masses would be \n",
    "\n",
    "\\begin{equation}\n",
    "U = \\tfrac{1}{2} k_1 x_1^2 + \\tfrac{1}{2} k_{12} (x_1 - x_2)^2 + \\tfrac{1}{2} k_2 x_2^2 ,  \\hspace{50pt}  (12)  %%\\label{Ueq}\n",
    "\\end{equation}\n",
    "\n",
    "where $k_1$, $k_2$, and $k_{12}$ are the spring constants of the springs at the left, right, and center, respectively.  In addition to potential energy, the system will have kinetic energy due to motion of the masses:  \n",
    "\n",
    "\\begin{equation}\n",
    "T = \\tfrac{1}{2} m_1 \\dot{x}_1^2 + \\tfrac{1}{2} m_2 \\dot{x}_2^2 .   \\hspace{50pt}  (13)  %%\\label{Teq}\n",
    "\\end{equation}\n",
    "\n",
    "For a single object of mass $m$ attached to one spring with spring constant $k$, the equation of motion is $m\\ddot{x} = -kx$, or $m\\ddot{x} + kx = 0$.  For a set of two masses and springs as in the figure, the equations of motion (e.g., from Lagrange's equations) can be written in a similar form, as $\\mathbf{M} \\ddot{\\mathbf{x}} + \\mathbf{K} \\mathbf{x} = 0$, where \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x} =\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    " x_2\n",
    "\\end{pmatrix}\n",
    " , \\hspace{5mm} \\mathbf{M} = \n",
    "\\begin{pmatrix}\n",
    "m_1 & 0 \\\\\n",
    "0 & m_2\\\\\n",
    "\\end{pmatrix},\n",
    "\\hspace{5mm} \\text{and} \\hspace{5mm}\n",
    "\\mathbf{K} = \n",
    "\\begin{pmatrix}\n",
    "k_1 + k_{12} & -k_{12}       \\\\\n",
    "-k_{12}       & k_2 + k_{12} \\\\ \n",
    "\\end{pmatrix} .  \n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\mathbf{M}_{ij}$ is the coefficient in Eq. (13) of the term $\\dot{x}_{i} \\, \\dot{x}_{j}$, ignoring the factor of $\\tfrac{1}{2}$.  (Since there are no terms of the form $\\dot{x}_1 \\dot{x}_2$, the off-diagonal elements of $\\mathbf{M}$ are zero.)  Similarly, $\\mathbf{K}_{ij}$ is the coefficient in Eq. (12) of the term $x_{i} \\, x_{j}$ (again ignoring $\\tfrac{1}{2}$ factors).  Note that in Eq. (12), the term $\\tfrac{1}{2} k_{12} \\, (x_1 - x_2)^2$ expands to $\\tfrac{1}{2} k_{12} \\, x_1^2 + \\tfrac{1}{2} k_{12} \\, x_2^2 + \\tfrac{1}{2} (-2 k_{12}) \\, x_1 x_2$.  In constructing $\\mathbf{K}$, this last term is thought of as $-\\tfrac{1}{2} k_{12} (x_1 x_2 + x_2 x_1)$, and so the value $-k_{12}$ is assigned to both $\\mathbf{K}_{12}$ and $\\mathbf{K}_{21}$, making $\\mathbf{K}$ symmetric.\n",
    "\n",
    "In the usual way of solving such equations, we seek the ***normal modes*** of the system, and assume simple harmonic motion, i.e.\\ $x_1 = \\alpha_1 \\, e^{i (\\omega t - \\delta_1)} = a_1 \\, e^{i \\omega t}$ and $x_2 = \\alpha_2 \\, e^{i(\\omega t - \\delta_2)} = a_2 \\, e^{i\\omega t}$, which in matrix notation looks like $\\mathbf{x} = \\mathbf{a} \\, e^{i \\omega t}$ (where the $a_i$'s are complex, in general).  Substitution into $\\mathbf{M} \\ddot{\\mathbf{x}} + \\mathbf{K} \\mathbf{x} = 0$ then gives \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{K} \\mathbf{a} = \\omega^2 \\, \\mathbf{M} \\mathbf{a} .   \\hspace{50pt}  (14)   %\\label{osceq}\n",
    "\\end{equation}\n",
    "\n",
    "If the matrix $\\mathbf{M}$ were the identity matrix $\\mathbf{I}$ (with $1$'s along the diagonal and $0$'s everywhere else), this would be a straightforward eigenvalue equation, $\\mathbf{K} \\mathbf{a} = \\omega^2 \\, \\mathbf{I} \\mathbf{a} = \\omega^2 \\, \\mathbf{a}$, with $\\boldsymbol{\\omega}^2$ being an eigenvalue.  We would find the eigenvalues in the usual way, by solving the ***characteristic equation*** derived from the expression $|\\mathbf{K} - \\boldsymbol{\\omega}^2 \\mathbf{I}| = 0$ or, in a numerical approach, by computing the eigenvalues and eigenvectors of the matrix $\\mathbf{K}$.\n",
    "\n",
    "We can deal with the complicating $\\mathbf{M}$ factor in Eq. (14) by multiplying both sides by $\\mathbf{M}^{-1}$, assuming that $\\mathbf{M}$ is invertible. (In all of our applications, $\\mathbf{M}$ *will* be invertible since it will be diagonal, with the diagonal elements being the masses of the oscillating objects, none of which will be zero.  In this case, the inverse matrix is just the diagonal matrix whose  elements are the reciprocals of the masses in $\\mathbf{M}$.)  When we multiply Eq. (14) by $\\mathbf{M}^{-1}$, we get the new equation\n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \\mathbf{M}^{-1} \\mathbf{K} \\right) \\mathbf{a} = \\omega^2 \\, \\mathbf{M}^{-1} \\mathbf{M} \\, \\mathbf{a} = \\omega^2 \\, \\mathbf{I} \\mathbf{a} = \\omega^2 \\, \\mathbf{a} .  \n",
    "\\end{equation}\n",
    "\n",
    "This is now a standard eigenvalue equation, with $\\mathbf{M}^{-1} \\mathbf{K}$ as the matrix whose eigenvalues and eigenvectors we want to compute.  Code to do this for the example shown in Figure 1 is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import inv\n",
    "\n",
    "M = [[m1, 0.],[0., m2]]                   # mass matrix\n",
    "Minv = inv(M)                             # invert M\n",
    "K = [[k1 + k12, -k12], [-k12, k1 + k2]]   # spring-constant matrix\n",
    "A = dot(Minv,K)                           # matrix multiply M^-1 K\n",
    "e, V = eigh(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Breakpoint 2</b></font>: Construct the matrix $\\mathbf{M}^{-1} \\mathbf{K}$ for the system of two coupled masses discussed above, and write out the corresponding characteristic equation for $\\omega^2$, $\\left| \\mathbf{M}^{-1} \\mathbf{K} - \\boldsymbol{\\omega}^2 \\mathbf{I} \\right| = 0$ (but don't try to solve it). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"blue\">7.2.3  Energies of a Quantum System\n",
    "\n",
    "If you've had a course in quantum mechanics, you may remember that very few problems can be solved analytically for the energies and (eigen-) states of the system.  Generally, \"real world\" problems must be solved numerically, and in this section we'll see an example of how to do that.\n",
    "\n",
    "The solution starts with the idea that the wavefunction $\\Psi(x)$ for a particle or system can be expanded in a series of the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\Psi(x) = \\sum_{n} c_n \\, \\psi_n (x) ,   \\hspace{50pt}  (15)  %\\label{expansion}\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\psi_n$ are suitable ***basis functions*** for the particular problem and the $c_n$ are expansion coefficients.  These coefficients provide an alternate representation of the function $\\Psi$ since they (together with the fixed basis functions) fully capture that function.  (They are general versions of the coefficients in a Fourier series.)  The basis functions are designed to be orthonormal over some relevant range ($a$, $b$) so that\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_{a}^{b} \\psi_m (x) \\, \\psi_n (x) \\, dx = \n",
    "\\begin{cases}\n",
    "1  & \\text{if } m = n, \\\\\n",
    "0    & \\text{otherwise}.\n",
    "\\end{cases}    \\hspace{50pt}  (16)   %\\label{orthonorm}\n",
    "\\end{equation}\n",
    "\n",
    "In ***bra-ket notation***,$^2$ $\\Psi(x)$ is written as $\\left| \\Psi \\right>$, $\\psi_n (x)$ becomes $\\left| \\psi_n \\right>$, and the completeness of the set of basis functions (their ability to represent any other relevant function) is encapsulated in the expansion $\\hat{I} = \\sum_n \\left| \\psi_n \\right> \\left< \\psi_n \\right|$, where $\\hat{I}$ is the identity operator which, acting on a function, leaves it unchanged.  (In matrix form, $\\left| \\psi \\right> \\left< \\psi \\right|$ represents the outer product of a column vector and a row vector, which creates a matrix; the expression for $\\hat{I}$ then says that the identity matrix can be built from a sum of such outer products made of the basis functions in column and row vector forms.)  In this notation, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\left| \\Psi \\right> = \\hat{I} \\left| \\Psi \\right> = \\sum_n \\left| \\psi_n \\right> \\left< \\psi_n \\, \\right| \\left. \\Psi \\right> .   \\hspace{50pt}  (17)   %\\label{BKexpansion}\n",
    "\\end{equation}\n",
    " \n",
    "(This expression is equivalent to this one for normal vectors in three dimensions, $\\mathbf{v} = \\sum_i \\left( \\mathbf{v} \\cdot \\mathbf{e}_i \\right) \\mathbf{e}_i$, where the $ \\mathbf{e}_i$ represent unit vectors in the $x$, $y$, and $z$ directions.)  By comparison with Eq. (15) we can see that \n",
    "\n",
    "\\begin{equation}\n",
    "c_n = \\left< \\psi_n \\, \\right| \\left. \\Psi \\right> .   \\hspace{50pt}  (18)  %\\label{coeffs}\n",
    "\\end{equation}\n",
    "\n",
    "(The $c_n$ are like the components $v_i = \\left( \\mathbf{v} \\cdot \\mathbf{e}_i \\right)$ found by projecting a vector $\\mathbf{v}$ onto the unit  basis vectors.)  Now, an operator $\\hat{F}$ acting on our wavefunction transforms it into a new wavefunction according to $\\left| \\Psi' \\right> = \\hat{F} \\left| \\Psi \\right>$.  If we construct the inner product $\\left< \\psi_m \\, \\right| \\left. \\Psi' \\right> = \\left< \\psi_m \\right|$ $\\hat{F}$ $\\left| \\Psi \\right>$, and use the expansion of Eq. (17) between $\\hat{F}$ and $\\left| \\Psi \\right>$, we find\n",
    "\n",
    "\\begin{equation}\n",
    "\\left< \\psi_m \\, \\right| \\left. \\Psi' \\right> = \\sum_{n} \\left< \\psi_m \\left| \\hat{F} \\right| \\psi_n \\right> \\left< \\psi_n \\, \\right| \\left. \\Psi \\right> .  \n",
    "\\end{equation}\n",
    " \n",
    "The term containing $\\hat{F}$ we can think of as a row vector times a matrix times a column vector, so it's just a number, $F_{mn}$, and then using Eqs. (15) and (18) this expression reduces to\n",
    "\n",
    "\\begin{equation}\n",
    "c'_m = \\sum_{n} F_{mn} \\, c_n .  \n",
    "\\end{equation}\n",
    "\n",
    "This represents one row (the $m^{\\textrm{th}}$ one) of a matrix equation, $\\mathbf{c}' = \\mathbf{F}\\mathbf{c}$, in which a matrix $\\mathbf{F}$ (with elements $F_{mn}$) multiplies a column vector $\\mathbf{c}$ (with elements $c_n$) to produce a new column vector $\\mathbf{c}'$ (elements $c'_m$), and it shows how the operator $\\hat{F}$ alters the expansion coefficients of Eq. (15) when it transforms a wavefunction.  (It's important to emphasize that, in general, $c'_m \\ne c_m$.)\n",
    "\n",
    "The point of all of this is that if $\\hat{F}$ is chosen to be the ***Hamiltonian*** operator of a quantum system,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{H} = -\\dfrac{\\hbar^2}{2M} \\dfrac{d^2}{dx^2} + V(x) ,\n",
    "\\end{equation}\n",
    " \n",
    "representing the energy of a particle of mass $M$ in the potential $V(x)$, then the operator's eigenvalues are the allowed energies of the particle in that potential.  To find those eigenvalues, the matrix elements  $H_{mn} \\equiv \\left< \\psi_m \\right|$ $\\hat{H}$ $\\left| \\psi_n \\right>$ must be computed.  In traditional notation, this expression has the form\n",
    "\n",
    "\\begin{equation}\n",
    "H_{mn} = \\int_a^b \\psi_m(x) \\left[ -\\dfrac{\\hbar^2}{2M} \\dfrac{d^2}{dx^2} + V(x) \\right] \\psi_n(x) \\, dx ,\n",
    "\\end{equation}\n",
    "\n",
    "where $a$ and $b$ are the boundaries of the system's range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">**Recap**</font>\n",
    "\n",
    "* A leading method for finding eigenvalues and eigenvectors of a matrix is QR decomposition, which expresses the matrix as a product of an orthogonal matrix Q and an upper triangular matrix R.  \n",
    "<br>\n",
    "* Gram-Schmidt orthogonalization can be used to find the matrices Q and R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">**Reflection Prompts**</font>\n",
    "\n",
    "These questions are intended to help you think about what you learned from this module and how it might be useful to you in the future.  You are strongly encouraged to answer them before moving on to the next module.\n",
    "\n",
    "*  Which components of this module did you find you were easily able to work through, and why do you think they were especially easy for you? \n",
    "\n",
    "*  Which components of this module did you find more difficult to work through, and why do you think they were challenging?\n",
    "\n",
    "*  When you got stuck, what did you do to get unstuck?  Could this or similar actions be helpful if you get stuck in future work?\n",
    "\n",
    "*  What do you understand more deeply about this material?\n",
    "\n",
    "*  What questions or uncertainties remain for you regarding this material?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Exercises</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Exercise #1**</u>  \n",
    "Write a program to implement QR decomposition (steps 1-3 in the algorithm above). It should input a single matrix and output the individual matrices Q & R, and their product, QR.  Try it on the (symmetric) matrix below, and confirm that the product QR is the same matrix. Note: feel free to use `numpy`'s `dot` and `norm` functions. [Answer: the eigenvalues should be (21, 1, -3, -8).] } \n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "1  & 4  & 8  & 4   \\\\\n",
    "4  & 2  & 3  & 7   \\\\ \n",
    "8  & 3  & 6  & 9   \\\\\n",
    "4  & 7  & 9  & 2\n",
    "\\end{matrix}\n",
    "\\right) .\n",
    "\\end{equation}  \n",
    "\n",
    "\n",
    "<u>**Exercise #2**</u>  \n",
    "Write a new program that makes use of your QR decomposition of Exercise \\#1 to implement the remaining steps of the QR algorithm listed above.  Note: It might be useful to know that you can break out of a `for` loop using a combination of `if` and  `break` commands; for instance:\n",
    "\n",
    "```python\n",
    "for i in range(r): \n",
    "\tif [condition]: \n",
    "\t\tbreak \n",
    "```\n",
    "\n",
    "To break out of a *double* `for` loop, you can use the following construct:\n",
    "\n",
    "```python\n",
    "for i in range(r): \n",
    "\tfor j in range(s):  \n",
    "\t    if [condition]: \n",
    "\t        break  \n",
    "\telse:   \n",
    "\t    continue  \n",
    "\t        break \n",
    "```\n",
    "\n",
    "Try your program on the array of Exercise #1, and confirm its answers using `numpy`'s `eigh` function.  Also test your function using this matrix, and compare it to `eigh`'s output:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "4  & -1 & -1    \\\\\n",
    "-1 & 4  & -1    \\\\ \n",
    "-1 & -1 & 4    \\\\\n",
    "\\end{matrix}\n",
    "\\right) .\n",
    "\\end{equation}\n",
    "\n",
    "You should find that this matrix has a doubly-*degenerate* eigenvalue, i.e. two equal ones.  (A matrix can have more than two degenerate eigenvalues.)  Remember that when eigenvalues are degenerate, their corresponding eigenvectors are not uniquely defined -- they define a *space* of eigenvectors that all correspond to the same eigenvalue.  In such a situation, one can choose any orthogonal set of vectors within that space as eigenvectors corresponding to the degenerate eigenvalue.  Since the matrix above has a doubly-degenerate eigenvalue,  there is a two-dimensional space of vectors corresponding to that eigenvalue.  Therefore, while your program should give the *same* eigenvector as `eigh` does for the one non-degenerate eigenvalue, it probably will give eigenvectors for the degenerate eigenvalue that are *different* from those given by `eigh`.  As a partial check of whether your program has produced legitimate results, confirm that all three eigenvectors found by your program are orthogonal to each other.  \n",
    "<br>\n",
    "\n",
    "<u>**Exercise #3**</u>  \n",
    "The matrix below is the moment of inertia tensor $\\mathbf{I}$ for a cube of mass $M$ and side length $a$ rotating about a corner.  (The tensor is written in a coordinate system aligned with the edges of the cube.)  (a) Using an appropriate function, find the principal axes for the cube, and the corresponding moments of inertia (in terms of the coefficient $M a^2 / 12$).  One of the eigenvectors can be nicely rewritten in terms of $1/\\sqrt{3}$.  What direction does this eigenvector correspond to in the cube?  (b) Are the eigenvectors mutually orthogonal (all orthogonal to each other)?  Check this!  (c) Are all of the eigenvectors uniquely determined, or could you have gotten different ones?  If so, which could have been different?  Explain. (Hint: you might want to reread Exercise #2.) } \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{I} = \\dfrac{M a^2}{12} \\left( \n",
    "\\begin{matrix}\n",
    "8  & -3 & -3   \\\\\n",
    "-3 & 8  & -3   \\\\ \n",
    "-3 & -3 & 8    \\\\\n",
    "\\end{matrix}\n",
    "\\right) .\n",
    "\\end{equation}  \n",
    "\t\n",
    "\t\n",
    "<u>**Exercise #4**</u>  \n",
    "Two blocks, with masses $m_1$ and $m_2$, are connected in a linear arrangement by three springs, as in Figure 1.  Each mass is connected to a wall by a spring with spring constants $k_1$ and $k_2$, and the masses are connected to each other by a third spring with spring constant $k_{12}$.  Find the ***normal modes*** -- represented by the eigenvectors -- describing the motion of this system *in general*, and check it for the specific case that $m_1 = m_2 = 2.0$ and $k_1 = k_2 = k_{12} = 3.0$.  (You might want to wrap your code in a Python function, so that you can pass the masses and spring constants as arguments.)  What motions do the normal modes represent?  What are the oscillation frequencies corresponding to those modes?  Can you explain why the larger one is larger?  \n",
    "<br>\n",
    "\n",
    "<u>**Exercise #5**</u>  \n",
    "Construct a Python function to find the normal-mode frequencies ($\\omega$) and eigenvectors for an arbitrary set of masses connected by springs in a linear arrangement, with the springs at the ends attached to fixed points.  Apply your function to the particular case of three equal masses connected by four springs with equal spring constants, using the mass values and spring constants of Exercise #4.  \n",
    "<br>\n",
    "\n",
    "<u>**Exercise #6**</u>  \n",
    "A ***double pendulum*** consists of one pendulum bob (with mass $m_2$) hanging from one end of a massless, rigid rod (length $L_2$) whose other end is attached to another bob (mass $m_1$) that hangs from another massless, rigid rod (length $L_1$) attached to a fixed pivot point.  In terms of the angles of the two rods measured from the vertical, $\\phi_1$ and $\\phi_2$, the kinetic and potential energies of the double pendulum are \n",
    "\n",
    "\\begin{align}\n",
    "T & = \\tfrac{1}{2} \\left( m_1 + m_2 \\right) L_1^2 \\, \\dot{\\phi}_1^2 \\, + \\, m_2 \\, L_1 \\, L_2 \\, \\dot{\\phi}_1 \\, \\dot{\\phi}_2 \\, + \\, \\tfrac{1}{2} \\, m_2 \\, L_2^2 \\, \\dot{\\phi}_2^2 , \\\\\n",
    "U & = \\tfrac{1}{2} \\left( m_1 + m_2 \\right) g \\, L_1 \\, \\phi_1^2 \\, + \\, \\tfrac{1}{2} \\, m_2 \\, g \\, L_2 \\, \\phi_2^2 .\n",
    "\\end{align}\n",
    "\n",
    "(a) Analytically construct the $\\mathbf{M}$ and $\\mathbf{K}$ matrices for this scenario.  Check whether $\\mathbf{M}^{-1} \\mathbf{K}$ is symmetric when $m_1 = m_2$, $L_1 = L_2$.  (b) Write a Python function that takes two masses and lengths as inputs and computes the eigenfrequencies (the square roots of the eigenvalues) and eigenvectors of the double pendulum, using the appropriate `numpy.linalg` function.  (c) Try your function for the case that $m_1 = m_2 = 2.5$, $L_1 = L_2 = 4.0$.  You should be able to relate the eigenvector components within one eigenvector to each other through a factor of $\\sqrt{2}$.  Describe the motions that the eigenvectors represent.  \n",
    "<br>\n",
    "\n",
    "<u>**Exercise #7**</u>  \n",
    "In a course on quantum mechanics, you probably investigated Schrodinger's equation for the infinite one-dimensional potential well, with $V(x) = 0$ (for $0 < x < L$) and $V(x) = \\infty$ (for $x \\le 0$ and $L \\le x$). That problem can be solved analytically, but the case of *general* $V(x)$ (for $0 < x < L$) must be solved numerically. Since the wavefunction in the well, $\\Psi(x)$, must vanish at $x = 0$ and $x = L$, the typical normalized basis functions are $\\psi_n (x) = \\sqrt{\\tfrac{2}{L}} \\sin \\left( n \\dfrac{\\pi x}{L}\\right )$, with $n$ a positive integer.  Consider the specific case of a linear potential, $V(x) = ax/L$.  The matrix elements to be computed are \n",
    "\t\n",
    "\\begin{equation}\n",
    "H_{mn} = \\dfrac{2}{L} \\int_{0}^{L} \\sin \\left( m \\dfrac{\\pi x}{L} \\right) \\left[ -\\dfrac{\\hbar^2}{2M} \\dfrac{d^2}{dx^2} + \\dfrac{ax}{L} \\right] \\sin \\left( n \\dfrac{\\pi x}{L} \\right) \\, dx .\n",
    "\\end{equation}\n",
    " \t\n",
    "(a) Expand the integral above and use the orthonormality of the $\\sin$ functions [the appropriate version of Eq. (16)] and the expression below to find a general expression for $H_{mn}$ \n",
    " \t\n",
    "\\begin{equation}\n",
    "\\int_{0}^{L} x \\sin \\left( m \\dfrac{\\pi x}{L}\\right ) \\sin \\left( n \\dfrac{\\pi x}{L}\\right ) \\, dx = \n",
    "\\begin{cases}\n",
    "0, &  \\text{if } m \\ne n \\text{ and both are even or odd}; \\\\\n",
    "- \\left( \\dfrac{2L}{\\pi} \\right)^2 \\dfrac{mn}{\\left(m^2 - n^2 \\right)^2}, & \\text{if } m \\ne n \\text{ and one is even, one odd}; \\\\\n",
    "L^2/4, & \\text{if }  m = n .\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "    \n",
    "(b) Write a Python function to evaluate the expression you found in (a) for an electron, arbitrary $m$ and $n$, and for $L = 0.5$ nm, $a = 10$ eV.  Be careful with units, and remember that the eigenvalue expansions involving $\\sin(n \\pi x/L)$ start at $n = 1$, but Python's array indices start with $0$.  [Python reminder: you can create an array of all zeros using the `zeros` function.] \n",
    "\n",
    "(c) Using your function from (b), and your function for finding the eigenvalues of a matrix (or `numpy`'s `eigh` or `eigvalsh` functions), compute the first ten eigenvalues of the matrix $H_{mn}$.  (You'll want to use just the first $10 \\times 10$ portion of the infinite matrix, which will yield an approximation to the eigenvalues.)  [Check:The ground state energy is $5.84$ eV.] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3pt\">**Breakpoint Answers**</font>\n",
    "\n",
    "**Breakpoint 1**:  \n",
    "Start by choosing one of the vectors to be $\\mathbf{v}_1$ and normalizing it: \n",
    "$$\\mathbf{v}_1 = \\dfrac{\\mathbf{u}_1}{\\left| \\mathbf{u}_1 \\right|} = \\dfrac{1}{\\sqrt{3}}(1,-1,1).$$\n",
    "\n",
    "Next, compute the projection of $\\mathbf{u}_2 = (1,0,1)$ on $\\mathbf{v}_1$ and subtract the result from $\\mathbf{u}_2$; then normalize the new result:\n",
    "$$\\mathbf{u}_2 - (\\mathbf{u}_2 \\bullet \\mathbf{v}_1) \\mathbf{v}_1 = (1,0,1) - \\left[ (1,0,1) \\bullet \\dfrac{1}{\\sqrt{3}}(1,-1,1) \\right] \\cdot  \\dfrac{1}{\\sqrt{3}}(1,-1,1) = \\dfrac{1}{3}(1,2,1),$$\n",
    "$$\\mathbf{v}_2 = \\dfrac{\\tfrac{1}{3}(1,2,1)}{\\sqrt{\\tfrac{1}{9} \\cdot 6}} =  \\dfrac{1}{\\sqrt{6}}(1,2,1).$$\n",
    "\n",
    "Follow the same procedure with $\\mathbf{u}_3 = (1,1,2)$, subtracting its projections on both $\\mathbf{v}_1$ and $\\mathbf{v}_2$, find:\n",
    "\\begin{align*}\n",
    "\\mathbf{u}_3 - (\\mathbf{u}_3 \\cdot \\mathbf{v}_1) \\mathbf{v}_1 - (\\mathbf{u}_3 \\cdot \\mathbf{v}_2) \\mathbf{v}_2 & = (1,1,2) - \\left[  (1,1,2) \\bullet \\dfrac{1}{\\sqrt{3}}(1,-1,1) \\right] \\cdot \\dfrac{1}{\\sqrt{3}}(1,-1,1) \\\\\n",
    "& - \\left[ (1,1,2) \\bullet \\dfrac{1}{\\sqrt{6}}(1,2,1) \\right] \\cdot  \\dfrac{1}{\\sqrt{6}}(1,2,1) = \\dfrac{1}{2}(-1,0,1),  \\\\\n",
    "\\mathbf{v}_3 & = \\dfrac{\\tfrac{1}{2}(-1,0,1)}{\\sqrt{\\tfrac{1}{4} \\cdot 2}} = \\dfrac{1}{\\sqrt{2}}(-1,0,1).\n",
    "\\end{align*}\n",
    "\n",
    "It's easy to confirm that these three vectors are orthonormal.  \n",
    "<br>\n",
    "\n",
    "**Breakpoint 2**:  \n",
    "$\\mathbf{M}^{-1}$ is simply \n",
    "$\n",
    "\\begin{pmatrix}\n",
    "m_1^{-1} & 0 \\\\\n",
    "0 & m_2^{-1} \n",
    "\\end{pmatrix}\n",
    "$,\n",
    "\n",
    "so \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{M}^{-1} \\mathbf{K} = \n",
    "\\begin{pmatrix}\n",
    "m_1^{-1} & 0 \\\\\n",
    "0 & m_2^{-1}\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "k_1 + k_{12} & -k_{12}       \\\\\n",
    "-k_{12}       & k_2 + k_{12} \n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\dfrac{k_1 + k_{12}}{m_1}  &  -\\dfrac{k_{12}}{m_1}  \\\\\n",
    "-\\dfrac{k_{12}}{m_2}   &  \\dfrac{k_2 + k_{12}}{m_2} \n",
    "\\end{pmatrix}.  \n",
    "\\end{equation}\n",
    "\n",
    "Therefore, the characteristic equation has the form \n",
    "\n",
    "\\begin{align}\n",
    "\\left| \\begin{pmatrix}\n",
    "\\dfrac{k_1 + k_{12}}{m_1}  &  -\\dfrac{k_{12}}{m_1}  \\\\\n",
    "-\\dfrac{k_{12}}{m_2}   &  \\dfrac{k_2 + k_{12}}{m_2} \n",
    "\\end{pmatrix} - \n",
    "\\begin{pmatrix}\n",
    "\\omega^2  &  0   \\\\\n",
    "0   &  \\omega^2  \n",
    "\\end{pmatrix}  \\right| \n",
    "& = \n",
    "\\begin{vmatrix}\n",
    "\\dfrac{k_1 + k_{12}}{m_1} - \\omega^2  &  -\\dfrac{k_{12}}{m_1}  \\\\\n",
    "-\\dfrac{k_{12}}{m_2}   &  \\dfrac{k_2 + k_{12}}{m_2} - \\omega^2\n",
    "\\end{vmatrix}  \\\\\n",
    "& = \n",
    "\\left( \\dfrac{k_1 + k_{12}}{m_1} - \\omega^2 \\right)  \\left( \\dfrac{k_2 + k_{12}}{m_1} - \\omega^2 \\right) - \\dfrac{k_{12}^2}{m_1 m_2} = 0.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3pt\">**References**</font>\n",
    "\n",
    "$^1$ Module adapted in part from *Computational Physics* by Mark Newman.\n",
    "\n",
    "$^2$ In this notation, a ***bra***, $\\left< \\phi \\right|$, essentially represents a row vector, and a ***ket***, $\\left| \\xi \\right>$, represents a column vector.  Their inner product then is $\\left< \\phi \\, \\right| \\left. \\xi \\right>$.  See an advanced quantum text for more background."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
