{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'> BRYN MAWR COLLEGE\n",
    "\n",
    "<font face=\"times\"><font size=\"6pt\"><p style = 'text-align: center;'><b>Computational Methods in the Physical Sciences</b><br/><br/>\n",
    "\n",
    "<p style = 'text-align: center;'><b>Module 6:  Solution of Linear Equations</b><br/><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Prerequisite modules***: Module 0; Module 1 \n",
    "\n",
    "***Estimated completion time***: 3-6 hours  \n",
    "\n",
    "***Learning objectives***: Understand the application of matrix methods to the solution of simultaneous, linear algebraic equations; apply these methods to least-squares fitting of data \n",
    "\n",
    "***Note:*** This module requires some familiarity with linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/matrix_transform_xkcd.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>(Image credit: xkcd.com)</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications that require the solution of simultaneous linear equations are common in science, engineering, and other quantitative fields.  Examples include solving for unknown circuit elements, determining forces on parts of a structure, analyzing a system of coupled oscillators, computing the action of a system of optical elements, and others.  This module will focus on a few of the many techniques for solving systems of linear equations.  These techniques, based heavily on matrix manipulations, also can be used for least-squares fitting of experimental data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Scientist Profile</font>\n",
    "\n",
    "**Dr. Ellen Ochoa** is an American astronaut and, since 2012, the director of NASA's Johnson Space Center in Houston, Texas.  Born in Los Angeles in 1958, she earned a Bachelor's degree in physics from San Diego State University in 1980, and a Masters and Ph.D. in electrical engineering from Stanford University (in 1981 and 1985, respectively).  She is the first Hispanic director and second woman director of the Johnson facility, and the first Hispanic woman to have gone into space.  Her first journey beyond Earth was a nine-day mission on the shuttle Discovery in 1993, which focused on observations to help understand the effect of solar activity on the Earth's climate and environment.  She returned to space three times thereafter, in 1994, 1999, and 2002.  As a Ph.D. student and later as a researcher at Sandia National Laboratories in New Mexico and at NASA Ames Research Center in California, Ochoa investigated optical systems for performing information processing, and she is the co-holder of three patents in the field.  Dr. Ochoa has been awarded NASA's Distinguished Service Medal, the Exceptional Service Medal, the Outstanding Leadership Medal, and four Space Flight Medals.  She also is a recipient of numerous other awards, including the Harvard Foundation Science Award, the Women in Aerospace Outstanding Achievement Award, The Hispanic Engineer Albert Baez Award for Outstanding Technical Contribution to Humanity, and the Hispanic Heritage Leadership Award.  (This profile adapted from http://www.nasa.gov/centers/johnson/about/people/orgs/bios/ochoa.html and http://www.jsc.nasa.gov/Bios/htmlbios/ochoa.html.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/Ellen_Ochoa.jpg' width=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">6.1  Gaussian elimination with back-substitution</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by considering a relatively simple scenario involving just three unknown variables $(x_1, x_2, x_3)$ whose relationships are represented by a set of simultaneous linear equations$^1$ of the form\n",
    "\n",
    "\\begin{align}   \n",
    "a_{1,1} \\, x_1 + a_{1,2} \\, x_2 + a_{1,3} \\, x_3 & = r_1 , \\\\\n",
    "a_{2,1} \\, x_1 + a_{2,2} \\, x_2 + a_{2,3} \\, x_3 & = r_2 , \\\\\n",
    "a_{3,1} \\, x_1 + a_{3,2} \\, x_2 + a_{3,3} \\, x_3 & = r_3 .\n",
    "\\end{align} \n",
    "\n",
    "The set of coefficients $a_{i, j}$ can be represented as a matrix $\\mathbf{A}$, and the constants $r_i$ as a vector $\\mathbf{r}$; all of these constants are assumed known, and we wish to solve for the set of unknown variables $x_i$ ($i = 1,2,3$) which can be represented as another vector, $\\mathbf{x}$.  This set of equations thus can be written in matrix form as $\\mathbf{A} \\mathbf{x} = \\mathbf{r}$.\n",
    "\n",
    "The standard way to solve such a matrix equation, at least in principle, is to invert the matrix $\\mathbf{A}$, and then compute the $x_i$ from $\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{r}$.  It turns out, though, that numerical matrix inversion is inefficient, and that there are superior methods for solving such an equation, at least in the case of a square matrix $\\mathbf{A}$.  (Non-square rectangular $\\mathbf{A}$'s can arise, e.g., in the case of fitting a function to a set of data, in which there may be more equations than unknowns.  This situation will be considered in the next module.)  A common method of solving the matrix equation is ***Gaussian elimination***, which you may be familiar with from a linear algebra course.  Gaussian elimination makes use of two operations that do not change the solutions $x_i$: (i) Any of the equations can be multiplied by any constant; (ii) Any linear combination of two of the equations can be made into another valid equation.  (See any linear algebra text for proofs of these facts.)  \n",
    "\n",
    "The Gaussian elimination procedure uses these two facts to change the matrix $\\mathbf{A}$ into one having ***upper triangular*** form, such that all elements below the diagonal are zero (additionally, in the case of Gaussian elimination the diagonal elements are 1).  For conciseness, the procedure will be described below in terms of the generic parameters $a_{i,j}$, $x_i$ and $r_i$.  (A specific example is presented below.)  Note that in this procedure the values $x_i$ are not altered; only the values of the parameters $a_{i,j}$ and $r_i$ are modified.  Also keep in mind that while the problem presented above involves three equations, and therefore 3-element vectors and a 3 $\\times$ 3 matrix, the procedure below can be applied to any number of equations ($N$) for the same number of unknowns.\n",
    "\n",
    "\n",
    "1. Divide the first row of $\\mathbf{A}$, and of $\\mathbf{r}$, by the value $a_{1,1}$ (known as the ***pivot*** for this step), so as to make the new first coefficient, $a'_{1, 1}$, equal to 1.  The other first-row parameters then become $a'_{1,2} = \\dfrac{a_{1,2}}{a_{1,1}}$, $a'_{1,3} = \\dfrac{a_{1,3}}{a_{1,1}}$, etc., and $r'_1 = \\dfrac{r_1}{a_{1,1}}$.    \n",
    "\n",
    "2. Multiply the first row by $a_{2,1}$ and subtract the resulting row from the second row to get a new second row with a leading coefficient $a'_{2,1} = 0$.  Apply this  procedure to each of the rows below the second one, so that the first column of the new matrix ends up as $(1, 0, 0, \\dots)$.    \n",
    "\t\n",
    "3. Now basically repeat step 1, but starting with the *second* row: divide the second row of parameters by the value in its *second* column (the new pivot, in the position of what had been $a_{2,2}$) so that the value at this position becomes $1$. \n",
    "\t\n",
    "4. As in step 2, multiply the second row in turn by the value in the second position in each of the subsequent rows and subtract the corresponding result from each of those rows; this leaves each of them with the value zero in the second position.  \n",
    "\t\n",
    "5. Repeat steps 3 and 4 until the matrix has been converted to upper triangular form (with $1$'s along the diagonal).\n",
    "\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterative procedure above can be expressed more concisely in a loop of just three steps (which almost serves as pseudocode for a program to implement Gaussian elimination):\n",
    "\n",
    "* Starting with $i = 1$ ($\\texttt{i = 0}$ in Python code), divide the $i^{\\textrm{th}}$ row of $\\mathbf{A}$ and the $i^{\\textrm{th}}$ element of $\\mathbf{r}$ by the value $a_{i,i}$ to make the new $i^{\\textrm{th}}$ coefficient, $a'_{i,i}$, equal to $1$.    \n",
    "\t\n",
    "* Looping over $j > i$, multiply the $i^{\\textrm{th}}$ row of $\\mathbf{A}$ and also $\\mathbf{r}_i$ by $a_{j, i}$ and subtract the resulting row from the $j^{\\textrm{th}}$ row (and the resulting $r$ from $r_j$) to get a new $j^{\\textrm{th}}$ row with new coefficient $a'_{j,i} = 0$ (and corresponding new $r_j$ value).    \n",
    "\t\n",
    "* While $i < N$ (the number of rows in the matrix), increment $i$ by $1$ and return to step 1.    \n",
    "\t\t\t\t\t\n",
    "\n",
    "As a particular example, consider the equation\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "1 & 2 & 2   \\\\\n",
    "4 & 4 & 2  \\\\\n",
    "4 & 6 & 4     \n",
    "\\end{matrix} \n",
    "\\right) \n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "x_1   \\\\\n",
    "x_2  \\\\\n",
    "x_3     \n",
    "\\end{matrix} \n",
    "\\right) = \n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "3   \\\\\n",
    "6  \\\\\n",
    "10     \n",
    "\\end{matrix} \n",
    "\\right).   \n",
    "\\end{equation*}\n",
    " \n",
    "The first element of the first row of the matrix, $a_{1,1}$, already is $1$, so we don't need to divide by it.  The next step, then, is to multiply the first row (of both the matrix and the numerical vector) by $4$ and subtract the result from the second row (of both matrix and vector).  The results are\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "1 & 2 & 2   \\\\\n",
    "0 & -4 & -6  \\\\\n",
    "4 & 6 & 4     \n",
    "\\end{matrix} \n",
    "\\right) \\hspace{15mm} \\textrm{and} \\hspace{15mm} \n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "3   \\\\\n",
    "-6  \\\\\n",
    "10     \n",
    "\\end{matrix} \n",
    "\\right) .\n",
    "\\end{equation*}\n",
    "\n",
    "Next, we again multiply the first row by $4$ and subtract it from the *third* row, to make *its* first element a zero.  We get\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "1 & 2 & 2   \\\\\n",
    "0 & -4 & -6  \\\\\n",
    "0 & -2 & -4     \n",
    "\\end{matrix} \n",
    "\\right) \\hspace{15mm} \\textrm{and} \\hspace{15mm} \n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "3   \\\\\n",
    "-6  \\\\\n",
    "-2     \n",
    "\\end{matrix} \n",
    "\\right) .\n",
    "\\end{equation*}\n",
    "\n",
    "We next move to the second row and divide it by the leading non-zero term, $-4$, obtaining\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "1 & 2 & 2   \\\\\n",
    "0 & 1 & 1.5  \\\\\n",
    "0 & -2 & -4     \n",
    "\\end{matrix} \n",
    "\\right) \\hspace{15mm} \\textrm{and} \\hspace{15mm} \n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "3   \\\\\n",
    "1.5  \\\\\n",
    "-2     \n",
    "\\end{matrix} \n",
    "\\right) .\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Breakpoint 1</b></font>: Complete the Gaussian elimination process for this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After going through this procedure, the matrix equation (for three simultaneous equations) will have the general form \n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "1 & c_{1,2} & c_{1,3}  \\\\\n",
    "0 & 1 & c_{2,3}  \\\\ \n",
    "0 & 0 & 1\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "= \n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "v_3\n",
    "\\end{matrix}\n",
    "\\right) ,  \\hspace{50pt}  (1)  %\\label{UTxv}\n",
    "\\end{equation}\n",
    "\n",
    "where the $v_i$ are produced from the $r_i$ as a result of the transformations outlined above.  This equation could be written in matrix form as $\\mathbf{U} \\mathbf{x} = \\mathbf{v}$ (where $\\mathbf{U}$ is for upper-triangular).\n",
    "\n",
    "Given a matrix in upper-triangular form with 1's on the diagonal, the rest of the solution now is straightforward.  The method is known as ***back-substitution***, and the key is to work upward from the last row. We can see immediately that the third row of Eq. (1) indicates that $x_3 = v_3$.  Next, matrix multiplication using the second row of the matrix gives $x_2 + c_{2,3} \\, x_3 = v_2$; substituting $v_3$ for $x_3$ we find $x_2 = v_2 - c_{2,3} \\, v_3$.  Finally, the first row of the matrix equation implies $x_1 + c_{1,2} \\, x_2 + c_{1,2} \\, x_3 = v_1$, so $x_1 = v_1 - c_{1,2} \\, x_2 - c_{1,3} \\, x_3$, and we can solve for $x_1$ by substituting the values for $x_2$ and $x_3$ that we just found.  The process in the case of more than three equations is the same; there simply are more substitutions to perform.  In fact, we note that we can write each result for $x_i$ generally as \n",
    "\n",
    "\\begin{equation}\n",
    "x_i = v_i - \\sum_{j>i} c_{i,j} \\, x_j .    \\hspace{50pt}  (2)  %\\label{backsub}\n",
    "\\end{equation}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Breakpoint 2</b></font>: Use back-substitution and your result for **Breakpoint 1** to solve for the $x_i$ values in the example above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potential problem may have occurred to the attentive reader: if the first element of the first row of the matrix of values, $a_{0,0}$, is zero, then the Gaussian elimination process fails at the start, since it would require dividing by zero.  An easy fix is to swap the first row with some other row that does not have this shortcoming.  This process is known as ***pivoting***.  However, since the matrix elements change as the Gaussian elimination procedure progresses, similar problems can arise throughout the process.  A general solution is ***partial pivoting***: when processing the $i^{th}$ row (in step 1 of the three-step procedure outlined above), first compare the absolute value of its $i^{th}$ element with the absolute value of the $i^{th}$ element in each lower row; if the largest such value is not the one in the $i^{th}$ row, then swap the $i^{th}$ row with the row that has the largest $i^{th}$ absolute value. (The reason for using the row with the largest absolute value is to avoid, as much as possible, dividing by a small element, which could lead to numerical errors.)  The rest of the procedure then follows as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"><b>Breakpoint 3</b></font>: Consider the two matrices shown below.  For each one, say whether partial pivoting should be used, based on the discussion above.  If so, which rows should be interchanged?  If not, explain why not.\n",
    "\\begin{equation*}\n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "1  &  -1  &  3   \\\\\n",
    "0  &  3  &  6     \\\\ \n",
    "0  &  1  &  -2    \n",
    "\\end{matrix}\n",
    "\\right)   \\hspace{10mm}  \\textrm{and}  \\hspace{10mm} \n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "1  &  -1  &  3   \\\\\n",
    "0  &  3  &  2     \\\\ \n",
    "0  &  -6  &  5    \n",
    "\\end{matrix}\n",
    "\\right)  \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">6.2  LU decomposition</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In physics applications, we often want to solve sets of equations of the form $\\mathbf{A} \\mathbf{x} = \\mathbf{r}$ in which $\\mathbf{A}$ remains the same but $\\mathbf{r}$ changes.  An example might be changing one or more of the batteries in a circuit, but keeping the resistors the same.  In these situations, there is no need to repeat the entire Gaussian elimination procedure each time, as $\\mathbf{A}$ would be transformed in the same way in each case.  A more efficient solution method in such situations, called  ***LU decomposition***, makes use of the fact that the steps of the earlier procedure all can be represented by matrix multiplications.  (A reader who is not interested in the details of this approach can skip to the paragraph immediately above the small bit of Python code below.)\n",
    "\n",
    "For example, recall that in the first step we divide the first row by $a_{1,1}$; we then subtract $a_{2,2}$ times the new first row from row 2, subtract $a_{3,3}$ times the new first row from row 3, etc.  This part of the process can be implemented for a 4 $\\times$ 4 matrix $\\mathbf{A}$ (with elements $a_{i,j}$) by the matrix multiplication indicated below\n",
    "\n",
    "\\begin{equation}\n",
    "\\dfrac{1}{a_{1,1}} \\left( \n",
    "\\begin{matrix}\n",
    "   1       & 0       & 0       & 0       \\\\\n",
    "  -a_{2,1} & a_{1,1} & 0       & 0       \\\\ \n",
    "  -a_{3,1} & 0       & a_{1,1} & 0       \\\\\n",
    "  -a_{4,1} & 0       & 0       & a_{1,1}\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "  a_{1,1}  & a_{1,2}  & a_{1,3}  & a_{1,4}  \\\\\n",
    "  a_{2,1}  & a_{2,2}  & a_{2,3}  & a_{2,4}  \\\\ \n",
    "  a_{3,1}  & a_{3,2}  & a_{3,3}  & a_{3,4}  \\\\\n",
    "  a_{4,1}  & a_{4,2}  & a_{4,3}  & a_{4,4}\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "= \n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "  1  & b_{1,2}  & b_{1,3}  & b_{1,4}  \\\\\n",
    "  0  & b_{2,2}  & b_{2,3}  & b_{2,4}  \\\\ \n",
    "  0  & b_{3,2}  & b_{3,3}  & b_{3,4}  \\\\\n",
    "  0  & b_{4,2}  & b_{4,3}  & b_{4,4}\n",
    "\\end{matrix}\n",
    "\\right) ,   \\hspace{50pt}  (3)   %\\label{t1a}\n",
    "\\end{equation}\n",
    "\n",
    "where the $b_{i,j}$ are new matrix elements determined by the multiplication.  We see that the first column has been converted into the desired form by multiplication by the first transformation matrix, which (including the initial factor of $1 / a_{1,1}$) will be denoted by $\\mathbf{T}_1$.  Thus, we can write the previous expression as $\\mathbf{T}_1 \\mathbf{A} = \\mathbf{B}$.  \n",
    "\n",
    "Another transformation matrix can be constructed to represent the next step of the Gaussian elimination process, in which row 2 of the new matrix, $\\mathbf{B}$, is divided by $b_{2,2}$ and then appropriate multiples of the resulting row are subtracted from each of the lower rows.  This second elimination step is implemented by a second transformation matrix, $\\mathbf{T}_2$, which appears here as the first matrix (together with the preceding factor $1 / b_{2,2}$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\dfrac{1}{b_{2,2}} \\left( \n",
    "\\begin{matrix}\n",
    "  b_{2,2}  & 0         & 0       & 0       \\\\\n",
    "  0        & 1         & 0       & 0       \\\\ \n",
    "  0        & -b_{3,2}  & b_{2,2} & 0       \\\\\n",
    "  0        & -b_{4,2}  & 0       & b_{2,2}\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "  1  & b_{1,2}  & b_{1,3}  & b_{1,4}  \\\\\n",
    "  0  & b_{2,2}  & b_{2,3}  & b_{2,4}  \\\\ \n",
    "  0  & b_{3,2}  & b_{3,3}  & b_{3,4}  \\\\\n",
    "  0  & b_{4,2}  & b_{4,3}  & b_{4,4}\n",
    "\\end{matrix}\n",
    "\\right) \n",
    "= \n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "  1  & c_{1,2}  & c_{1,3}  & c_{1,4}  \\\\\n",
    "  0  & 1        & c_{2,3}  & c_{2,4}  \\\\ \n",
    "  0  & 0        & c_{3,3}  & c_{3,4}  \\\\\n",
    "  0  & 0        & c_{4,3}  & c_{4,4}\n",
    "\\end{matrix}\n",
    "\\right) ,   \\hspace{50pt}  (4)  %\\label{t2b}\n",
    "\\end{equation}\n",
    "\n",
    "or $\\mathbf{T}_2 \\mathbf{B} = \\mathbf{C}$.  In matrix notation we so far have $\\mathbf{T}_2 \\mathbf{T}_1 \\mathbf{A} = \\mathbf{C}$.  A similar procedure applied to the third row of matrix $\\mathbf{C}$, and then to the fourth row of the matrix that it is transformed into ($\\mathbf{D}$), requires two additional transformation matrices, $\\mathbf{T}_3$ and $\\mathbf{T}_4$, shown below:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T}_3 = \\dfrac{1}{c_{3,3}} \n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "  c_{3,3}  & 0        & 0         & 0   \\\\\n",
    "  0        & c_{3,3}  & 0         & 0   \\\\ \n",
    "  0        & 0        & 1         & 0   \\\\\n",
    "  0        & 0        & -c_{4,3}  & c_{3,3}\n",
    "\\end{matrix}\n",
    "\\right) \\hspace{5mm} ; \\hspace{5mm}\n",
    "\\mathbf{T}_4 = \\dfrac{1}{d_{4,4}}\n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "  d_{4,4}  & 0       & 0        & 0  \\\\\n",
    "  0        & d_{4,4} & 0        & 0  \\\\ \n",
    "  0        & 0       & d_{4,4}  & 0  \\\\\n",
    "  0        & 0       & 0        & 1\n",
    "\\end{matrix}\n",
    "\\right) .  \n",
    "\\end{equation}\n",
    " \n",
    "\n",
    "The complete Gaussian elimination process applied to matrix $\\mathbf{A}$ is represented by the matrix multiplication $\\mathbf{T}_4 \\mathbf{T}_3 \\mathbf{T}_2 \\mathbf{T}_1 \\mathbf{A}$, which will result in $\\mathbf{A}$ being converted into upper-triangular form.  (Note that all of the $\\mathbf{T}$ matrices are lower-triangular: they have only zeros above their diagonals.)  The original equation that we wanted to solve, $\\mathbf{A} \\mathbf{x} = \\mathbf{r}$, now can be modified to $\\mathbf{T}_4 \\mathbf{T}_3 \\mathbf{T}_2 \\mathbf{T}_1 \\mathbf{A} \\mathbf{x} = \\mathbf{T}_4 \\mathbf{T}_3 \\mathbf{T}_2 \\mathbf{T}_1 \\mathbf{r}$.  The matrix multiplying $\\mathbf{x}$ on the left will be upper-triangular, and on the right we would get a vector, so this equation is of the form of Eq. (1), and it can be solved by back-substitution.  The point of this long process is that we now know how to put $\\mathbf{A}$ into upper-triangular form regardless of what $\\mathbf{r}$ is, so if that vector changes then only the back-substitution part of the analysis must be done from scratch.  \n",
    "\n",
    "The actual computation in the LU decomposition approach differs slightly from the procedure just described.  It first involves defining two matrices:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{L} = \\mathbf{T}_1^{-1} \\mathbf{T}_2^{-1} \\mathbf{T}_3^{-1} \\mathbf{T}_4^{-1} , \\hspace{5mm}\n",
    "\\mathbf{U} = \\mathbf{T}_4 \\mathbf{T}_3 \\mathbf{T}_2 \\mathbf{T}_1 \\mathbf{A} .\n",
    "\\end{equation}\n",
    " \n",
    "Note that $\\mathbf{U}$ is just the upper-triangular matrix that will multiply $\\mathbf{x}$, as described above.  Furthermore, matrix multiplication reveals that $\\mathbf{L}$ is a lower-triangular matrix.  In fact, that matrix has the form\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{L} = \\left( \n",
    "\\begin{matrix}\n",
    "a_{1,1}  & 0        & 0        & 0   \\\\\n",
    "a_{2,1}  & b_{2,2}  & 0        & 0   \\\\ \n",
    "a_{3,1}  & b_{3,2}  & c_{3,3}  & 0   \\\\\n",
    "a_{4,1}  & b_{4,2}  & c_{4,3}  & d_{4,4}\n",
    "\\end{matrix}\n",
    "\\right) .\n",
    "\\end{equation}\n",
    " \n",
    "\n",
    "If we multiply the two matrices just defined, we get \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{L} \\mathbf{U} = \\mathbf{A} ,\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "which is the LU decomposition of $\\mathbf{A}$.  With this result, the original set of equations $\\mathbf{A} \\mathbf{x} = \\mathbf{r}$ can be rewritten as $\\mathbf{L} \\mathbf{U} \\mathbf{x} = \\mathbf{r}$.  The trick now is to write $\\mathbf{U} \\mathbf{x} = \\mathbf{y}$, so then $\\mathbf{L} \\mathbf{y} = \\mathbf{r}$.  The usefulness of these two expressions is that they both involve just one triangular matrix, as in Eq. (1), and therefore can be solved immediately by back-substitution.  One first solves for $\\mathbf{y}$ in the $\\mathbf{U} \\mathbf{x} = \\mathbf{y}$ equation, and that vector then is substituted into the $\\mathbf{L} \\mathbf{y} = \\mathbf{r}$ equation to obtain $\\mathbf{r}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LU decomposition is the most common method for solving simultaneous equations based on Gaussian elimination.  One can see how the LU decomposition process would go in general.  First compute $\\mathbf{T}_1 \\mathbf{A}$, where the form of $\\mathbf{T}_1$ is clear (in the case of any number of dimensions) from Eq. (3).   This gives the new matrix $\\mathbf{B}$, from which the elements of $\\mathbf{T}_2$ can be determined via generalization of Eq. (4).  Use this to compute $\\mathbf{T}_2 \\mathbf{T}_1 \\mathbf{C}$, from which the elements of $\\mathbf{C}$ needed in $\\mathbf{T}_3$ can be found, and so on.  Keep in mind that there must be as many $\\mathbf{T}$ matrices as there are equations to be solved.  In addition, partial pivoting generally should be used as part of the process to avoid problems with zero-valued diagonal elements.\n",
    "\n",
    "Fortunately, in Python (as in many other languages) there already exist functions for solving sets of simultaneous equations.  The function that performs LU decomposition and back-substitution is called `solve`, and is part of the `linalg` module within the `numpy` package (more information on it and other Python functions for solving sets of equations is at www.scipy.org).  It is called as follows:\n",
    "\n",
    "```python\n",
    "from numpy.linalg import solve\n",
    "x = solve(A,r)\n",
    "```\n",
    "\n",
    "It's worth briefly mentioning that the solution approach discussed above can be streamlined in a certain common physics situation; namely, when the matrix $\\mathbf{A}$ of coefficients has a ***banded*** structure consisting of a main diagonal and a limited number of additional diagonals immediately above and below, with the elements farther from the diagonal being zero.  Here is a 4 $\\times$ 4 example, with one diagonal above and below the main one (making a ***tridiagonal*** matrix):\n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "a_{1,1}  & a_{1,2}  & 0        & 0   \t \\\\\n",
    "a_{2,1}  & a_{2,2}  & a_{2,3}  & 0   \t \\\\ \n",
    "0\t     & a_{3,2}  & a_{3,3}  & a_{3,4} \\\\\n",
    "0\t     & 0  \t    & a_{4,3}  & a_{4,4}\n",
    "\\end{matrix}\n",
    "\\right) .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This sort of situation arises, for example, in the case of a linear arrangement of masses connected by springs, and in similar scenarios in which an element of a system is affected by nearby neighbors but not more distant ones (e.g., the famous Ising model for a lattice of interacting spins).  The streamlining possible in this case occurs in step 3 of the three-step outline of the general Gaussian elimination process.  In the general case, each row needs to be multiplied by appropriate $a_{j,i}$'s and then subtracted from *each* of the rows below it.  In the case of a banded matrix with $n$ diagonals above and below the main one, each row needs to be appropriately multiplied and then subtracted only from the $n$ rows immediately below it.  For large matrices with narrow bands, this simplification can considerably reduce the computation time needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">6.3  Other decompositions</font>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LU decomposition is not the only possible factorization of a matrix, and other decompositions can be more efficient or otherwise superior in certain situations.  For example, in the case of a Hermitian, positive-definite matrix $\\mathbf{A}$ (for a real matrix, this means it's symmetric, with all positive eigenvalues), the ***Cholesky decomposition*** will run in about half the time of the LU decomposition.  This decomposition involves factoring the matrix $\\mathbf{A}$ into the product of a lower-triangular matrix and its (conjugate) transpose, i.e. $\\mathbf{A} = \\mathbf{LL}^T$, which sometimes is described as (sort of) taking the square root of a matrix.  It turns out that the Cholesky method does not even require any pivoting.  Another factorization in the same vein is the ***QR decomposition***, in which the matrix $\\mathbf{A}$ is factored into the form $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$, where $\\mathbf{Q}$ is orthogonal ($\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{1}$, where $\\mathbf{1}$ is the identity matrix) and $\\mathbf{R}$ is upper-triangular (what we labeled as $\\mathbf{U}$ earlier).  This is used for factorizing real matrices in ***Hessenberg form***: matrices with zeros everywhere below the main diagonal except for the diagonal just below the main one. More information on these decompositions can be found in advanced texts on numerical methods.\n",
    "\n",
    "Finally, a powerful method known as ***singular value decomposition*** may work even in the case of a matrix that is *singular* (having a zero determinant) or nearly so, when the Gaussian procedure fails.  It also is the favored method for solving linear least-squares problems.  This factorization method will be discussed again in Module 7.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">**Recap**</font>\n",
    "\n",
    "\n",
    "* Gaussian elimination with back-substitution is commonly used for numerically solving the matrix equation $\\mathbf{A} \\mathbf{x} = \\mathbf{r}$.  Partial pivoting is used to avoid numerical errors associated with division by small (or zero) matrix entries.  \n",
    "<br>\n",
    "\n",
    "* LU decomposition efficiently solves such problems in cases when $\\mathbf{r}$ varies but $\\mathbf{A}$ does not.  Other decompositions may be preferred for matrices with specific forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">**Reflection Prompts**</font>\n",
    "\n",
    "These questions are intended to help you think about what you learned from this module and how it might be useful to you in the future.  You are strongly encouraged to answer them before moving on to the next module.\n",
    "\n",
    "*  Which components of this module did you find you were easily able to work through, and why do you think they were especially easy for you? \n",
    "\n",
    "*  Which components of this module did you find more difficult to work through, and why do you think they were challenging?\n",
    "\n",
    "*  When you got stuck, what did you do to get unstuck?  Could this or similar actions be helpful if you get stuck in future work?\n",
    "\n",
    "*  What do you understand more deeply about this material?\n",
    "\n",
    "*  What questions or uncertainties remain for you regarding this material?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Exercises</font>\n",
    "\n",
    "<u>**Exercise #1**</u>  \n",
    "Write a Python function to implement the Gaussian elimination algorithm presented in the three-step loop shown above.  It should take a matrix of coefficients ($\\mathbf{A}$) and vector of results ($\\mathbf{r}$) as its inputs, and it should output an upper-triangular matrix ($\\mathbf{U}$) and corresponding vector ($\\mathbf{v}$).  Apply your function to the matrix below, and the vector $[-4., 3., 9., 7.]$, and check whether it yields an upper-triangular matrix with 1's along the diagonal.  Print out both the final matrix and final vector.   (Python reminders: `A[i,j]` denotes the `i-j` element of array `A`, `A[i]` denotes the entire row with index `i`, and `A[:,j]` denotes the entire column with index `j`.  Also remember that the first row/column has index `0`.) \n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "2. &  1. & 4.  & 1.  \\\\\n",
    "3. &  4. & -1. & -1. \\\\\n",
    "1. & -4. & 1.  & 5. \\\\ \n",
    "2. & -2. & 1.  & 3.  \n",
    "\\end{matrix} \n",
    "\\right) . \n",
    "\\end{equation}\n",
    "\n",
    "(Note: It is strongly recommended that $\\mathbf{A}$ and $\\mathbf{r}$ be implemented as *arrays* rather than lists in your function.  Answer: the output vector should be $[-2., 3.6, -2., 1.]$.)  \n",
    "<br>\n",
    "\n",
    "<u>**Exercise #2**</u>  \n",
    "Write a Python function to implement the back-substitution algorithm, Eq. (2).  Its inputs should be an upper-triangular array and a vector, and it should output a vector of $x_i$ values.  Run it on the array ($\\mathbf{U}$) and vector ($\\mathbf{v}$) from Exercise \\#1 to find the unknown $x_i$ values.  (Python tip: to create a loop that runs backwards, use the syntax `range[max, min-1, -1]`, where `max` is the largest index desired for the loop and `min` is the smallest index; the last `-1` reverses the loop.)  (Answer: $[2., -1., -2., 1.]$)  \n",
    "<br>\t\n",
    "\n",
    "<u>**Exercise #3**</u>  \n",
    "(a) Incorporate partial pivoting into the function you constructed in Exercise #1 above.  Print out the resulting matrix and vector.  (Note: to swap the rows with indices 1 and 2 in an array, you can use this syntax: `A[[1,2]] = A[[2,1]]`.)\n",
    "\n",
    "(b) Combine the Gaussian elimination (with partial pivoting) and back-substitution functions into a single function for solving simultaneous linear equations.  (This is not complicated: just call those two functions sequentially within a new function to which you pass the original matrix and vector, and pass the output of the Gaussian elimination function to the back-substitution function.)  Run it on the matrix of Exercise #1 and print out the solution vector to make sure the two parts work together properly.  \n",
    "<br>\t\n",
    "\t\n",
    "<u>**Exercise #4**</u>  \n",
    "The simple circuit below (Figure 1) can be analyzed by hand, but we will use it to further test our code.  The circuit contains two batteries (of $6$ and $12$ volts) and two resistors (of $6$ and $9$ ohms).  The *assumed* current directions in the three \"branches\" also are shown (as $I_1$, $I_2$, $I_3$).  Kirchhoff's ***loop rule*** states that the sum of voltage changes encountered in tracing a closed loop around the circuit must be zero.  That requirement gives the following equations for the left-hand and right-hand loops, respectively: \n",
    "\n",
    "\\begin{align}\n",
    "6 \\, I_1 + 0 \\, I_2 + 0 \\, I_3 & = -6 ,  \\\\\n",
    "0 \\, I_1 + 0 \\, I_2 + 9 \\, I_3 & = 12 ,  \n",
    "\\end{align} \n",
    "\t\n",
    "where the right-hand sides represent the battery voltages in each loop, and the current terms with zeros in front are included just to make the equation structure clear.  [The sign of a battery voltage depends on the direction in which the loop containing it is traversed relative to the battery's orientation.  The directions here have been chosen to make the current terms positive, but that's not necessary (or even always possible).  The $-6$ on the right-hand side of the first equation comes from the combination of the $6$-V and $12$-V batteries.]  \n",
    "\t\n",
    "We need one more equation to solve for the three unknown currents: that equation is found from Kirchhoff's ***junction rule***, which says that the net flow of current into a junction (where two or more branches meet) must be zero; i.e., the amount flowing in equals the amount flowing out.  Looking at the junction at the top of the circuit, we see $I_1$ and $I_2$ flowing in, and $I_3$ flowing out, so the junction rule equation would be $I_1 + I_2 - I_3 = 0$.  Combining this equation with the other two, we have \n",
    "\t\n",
    "\\begin{align}\n",
    "6 \\, I_1 + 0 \\, I_2 + 0 \\, I_3 & = -6,  \\\\\n",
    "0 \\, I_1 + 0 \\, I_2 + 9 \\, I_3 & = 12,  \\\\\n",
    "1 \\, I_1 + 1 \\, I_2 - 1 \\, I_3 & = 0.\n",
    "\\end{align}\n",
    "\t\n",
    "(a) Use your Gaussian elimination function (*without* partial pivoting) and your back-substitution function to analyze this problem to find the unknown currents.  You should find that you get error messages and nonsense output [the symbol \"nan\" (often written as \"NaN\") means \"not a number\"].  Why is that? \n",
    "\t\n",
    "(b) Use your combined function (incorporating partial pivoting) from Exercise #3 part (b) to determine the currents.  [Answer: $I_1 = -1.00$ amps, $I_2 = 2.33$ amps, $I_3 = 1.33$ amps; a negative current just means the actual direction is opposite what was assumed in drawing that current in the figure.] }\n",
    "\t\n",
    "Of course, something like the above approach can, and in fact must, be used in more complicated circuits that cannot be analyzed by hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './Images/Circuit_figure.png' width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Figure 1**: A simple circuit</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3pt\">**Breakpoint Answers**</font>\n",
    "\n",
    "**Breakpoint 1**  \n",
    "Multiply the second row by $-2$ and subtract it from the bottom row to get\n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "1 & 2 & 2   \\\\\n",
    "0 & 1 & 1.5  \\\\\n",
    "0 & 0 & -1     \n",
    "\\end{matrix} \n",
    "\\right) \\hspace{15mm} \\textrm{and} \\hspace{15mm} \n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "3   \\\\\n",
    "1.5  \\\\\n",
    "1     \n",
    "\\end{matrix} \n",
    "\\right) .\n",
    "\\end{equation}\n",
    "\n",
    "Finally, divide the last row by $-1$ to obtain\n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "1 & 2 & 2   \\\\\n",
    "0 & 1 & 1.5  \\\\\n",
    "0 & 0 & 1     \n",
    "\\end{matrix} \n",
    "\\right) \\hspace{15mm} \\textrm{and} \\hspace{15mm} \n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "3   \\\\\n",
    "1.5  \\\\\n",
    "-1     \n",
    "\\end{matrix} \n",
    "\\right) .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Breakpoint 2**  \n",
    "From the result above, we have \n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "1 & 2 & 2   \\\\\n",
    "0 & 1 & 1.5  \\\\\n",
    "0 & 0 & 1     \n",
    "\\end{matrix} \n",
    "\\right) \n",
    "\\left( \n",
    "\\begin{matrix} \n",
    "x_1  \\\\\n",
    "x_2  \\\\\n",
    "x_3\n",
    "\\end{matrix} \n",
    "\\right)  =  \n",
    "\\left( \n",
    "\\begin{matrix}  \n",
    "3   \\\\\n",
    "1.5  \\\\\n",
    "-1     \n",
    "\\end{matrix} \n",
    "\\right) .  \\\\\n",
    "\\end{equation}\n",
    "\n",
    "Immediately, we get $x_3 = -1$ from the bottom row of the matrix equation.  The second row gives $x_2 + 1.5\\, x_3 = 1.5$; substituting $x_3 = -1$ we find $x_2 = 3$.  Finally, the first row gives $x_1 +2 x_2 + 2 x_3 = 3$, and substitution of the other $x_i$ values yields $x_1 = -1$.  \n",
    "<br>\n",
    "\n",
    "\n",
    "**Breakpoint 3**  \n",
    "\\begin{equation}\n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "1  &  -1  &  3   \\\\\n",
    "0  &  3  &  6    \\\\ \n",
    "0  &  1  &  -2    \n",
    "\\end{matrix}\n",
    "\\right)   \n",
    "\\end{equation}\n",
    "\n",
    "This matrix does not require pivoting: the element ($1$) in the first row, first column is larger (in absolute value) than the other elements in the first column, and the element in the second row, second column ($3$) is larger than the element in the same column in the third row. \n",
    "\n",
    "\\begin{equation}\n",
    "\\left( \n",
    "\\begin{matrix}\n",
    "1  &  -1  &  3   \\\\\n",
    "0  &  3  &  2     \\\\ \n",
    "0  &  -6  &  5    \n",
    "\\end{matrix}\n",
    "\\right)  \n",
    "\\end{equation}\n",
    "\n",
    "This matrix should be pivoted based on the discussion in the text.  The element in the second row, second column ($3$) is smaller in absolute value than the element in the third row, second column ($-6$), so those two rows should be interchanged.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "$^1$ Most of this module is adapted from *Computational Physics* by Newman, with small contributions from *Scientific Computing* 2e by Heath."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
